{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè¶ Banking Sale Voice Chat - Colab\n",
        "\n",
        "Notebook n√†y ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng Banking Sale Voice Chat tr√™n Google Colab.\n",
        "\n",
        "**Ki·∫øn tr√∫c**: STT (Speech-to-Text) ‚Üí LLM ‚Üí TTS (Text-to-Speech)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "%pip install -q gradio>=5.42.0 transformers>=4.51.0 torch accelerate huggingface_hub openai-whisper edge-tts numpy scipy pydub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra GPU (n·∫øu c√≥)\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## T·∫£i code t·ª´ repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository ho·∫∑c t·∫£i file app.py\n",
        "import os\n",
        "\n",
        "# T·∫°o th∆∞ m·ª•c d·ª± √°n\n",
        "os.makedirs(\"banking_sale\", exist_ok=True)\n",
        "\n",
        "# T·∫£i app.py t·ª´ Hugging Face ho·∫∑c GitHub\n",
        "# B·∫°n c√≥ th·ªÉ thay ƒë·ªïi URL n√†y\n",
        "!wget -q -O banking_sale/app.py https://huggingface.co/spaces/hainguyen306201/banking_sale/resolve/main/app.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ho·∫∑c t·∫°o file app.py tr·ª±c ti·∫øp trong Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o file app.py\n",
        "app_code = '''\n",
        "import gradio as gr\n",
        "import torch\n",
        "import whisper\n",
        "import edge_tts\n",
        "import asyncio\n",
        "import io\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TextIteratorStreamer\n",
        "from threading import Thread\n",
        "from queue import Queue\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "\n",
        "# Load model v√† tokenizer\n",
        "MODEL_NAME = \"hainguyen306201/bank-model\"\n",
        "\n",
        "# Kh·ªüi t·∫°o model v√† tokenizer m·ªôt l·∫ßn khi app kh·ªüi ƒë·ªông\n",
        "model = None\n",
        "tokenizer = None\n",
        "whisper_model = None\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"Load t·∫•t c·∫£ models\"\"\"\n",
        "    global model, tokenizer, whisper_model\n",
        "    try:\n",
        "        print(\"\\n[1/2] ƒêang t·∫£i model bank-model...\")\n",
        "        print(\"      Model: hainguyen306201/bank-model\")\n",
        "        print(\"      ƒêi·ªÅu n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            low_cpu_mem_usage=True,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "        print(\"      ‚úÖ Model bank-model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
        "        \n",
        "        # Load Whisper model cho STT\n",
        "        print(\"\\n[2/2] ƒêang t·∫£i Whisper model cho STT...\")\n",
        "        print(\"      Model: openai/whisper-base\")\n",
        "        print(\"      ƒêi·ªÅu n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t...\")\n",
        "        whisper_model = whisper.load_model(\"base\")\n",
        "        print(\"      ‚úÖ Whisper model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå L·ªói khi t·∫£i model: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Load models - ƒê·ª£i t·∫£i xong tr∆∞·ªõc khi ti·∫øp t·ª•c\n",
        "print(\"=\" * 50)\n",
        "print(\"B·∫ÆT ƒê·∫¶U T·∫¢I MODELS...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "models_loaded = load_models()\n",
        "\n",
        "if models_loaded:\n",
        "    print(\"=\" * 50)\n",
        "    print(\"‚úÖ T·∫§T C·∫¢ MODELS ƒê√É ƒê∆Ø·ª¢C T·∫¢I TH√ÄNH C√îNG!\")\n",
        "    print(\"=\" * 50)\n",
        "else:\n",
        "    print(\"=\" * 50)\n",
        "    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: C√ì L·ªñI KHI T·∫¢I MODELS!\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# System message m·∫∑c ƒë·ªãnh\n",
        "DEFAULT_SYSTEM_MESSAGE = \"You are a helpful banking and finance assistant specialized in providing financial advice and banking services information. Respond in Vietnamese when the user speaks Vietnamese.\"\n",
        "\n",
        "\n",
        "def speech_to_text(audio):\n",
        "    \"\"\"\n",
        "    STT: Chuy·ªÉn ƒë·ªïi audio th√†nh text\n",
        "    \"\"\"\n",
        "    if audio is None:\n",
        "        return None\n",
        "    \n",
        "    if whisper_model is None:\n",
        "        return \"L·ªói: Whisper model ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng ƒë·ª£i...\"\n",
        "    \n",
        "    try:\n",
        "        # Whisper x·ª≠ l√Ω audio file\n",
        "        result = whisper_model.transcribe(audio, language=\"vi\")\n",
        "        text = result[\"text\"].strip()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói STT: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_response_stream(\n",
        "    message,\n",
        "    history: list,\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    \"\"\"\n",
        "    LLM: T·∫°o response t·ª´ LLM v·ªõi streaming\n",
        "    \"\"\"\n",
        "    if model is None or tokenizer is None:\n",
        "        yield \"L·ªói: Model ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng ƒë·ª£i ho·∫∑c refresh trang...\"\n",
        "        return\n",
        "    \n",
        "    # Chu·∫©n b·ªã messages\n",
        "    messages = []\n",
        "    if system_message:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "    else:\n",
        "        messages.append({\"role\": \"system\", \"content\": DEFAULT_SYSTEM_MESSAGE})\n",
        "    \n",
        "    # Th√™m l·ªãch s·ª≠ chat\n",
        "    messages.extend(history)\n",
        "    \n",
        "    # Th√™m message hi·ªán t·∫°i\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "    \n",
        "    # √Åp d·ª•ng chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Tokenize input\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # C·∫•u h√¨nh generation\n",
        "    generation_config = GenerationConfig(\n",
        "        max_new_tokens=min(max_tokens, 16384),\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=20,\n",
        "    )\n",
        "    \n",
        "    # T·∫°o streamer ƒë·ªÉ stream response theo th·ªùi gian th·ª±c\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    \n",
        "    # C·∫•u h√¨nh generation v·ªõi streamer\n",
        "    generation_kwargs = {\n",
        "        **model_inputs,\n",
        "        \"generation_config\": generation_config,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id,\n",
        "        \"streamer\": streamer,\n",
        "    }\n",
        "    \n",
        "    # Ch·∫°y generation trong thread ri√™ng\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "    \n",
        "    # Stream response\n",
        "    response_text = \"\"\n",
        "    for new_text in streamer:\n",
        "        response_text += new_text\n",
        "        yield response_text\n",
        "\n",
        "\n",
        "async def text_to_speech_async(text, voice=\"vi-VN-HoaiMyNeural\"):\n",
        "    \"\"\"\n",
        "    TTS: Chuy·ªÉn ƒë·ªïi text th√†nh audio (async)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # S·ª≠ d·ª•ng edge-tts ƒë·ªÉ t·∫°o audio\n",
        "        communicate = edge_tts.Communicate(text, voice)\n",
        "        audio_data = b\"\"\n",
        "        async for chunk in communicate.stream():\n",
        "            if chunk[\"type\"] == \"audio\":\n",
        "                audio_data += chunk[\"data\"]\n",
        "        \n",
        "        # T·∫°o file audio t·∫°m\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp_file:\n",
        "            tmp_file.write(audio_data)\n",
        "            return tmp_file.name\n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói TTS: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def text_to_speech(text):\n",
        "    \"\"\"\n",
        "    TTS wrapper (sync)\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    try:\n",
        "        return asyncio.run(text_to_speech_async(text))\n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói TTS: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_voice_input(\n",
        "    audio,\n",
        "    history: list,\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    \"\"\"\n",
        "    X·ª≠ l√Ω voice input: STT ‚Üí LLM ‚Üí TTS\n",
        "    \"\"\"\n",
        "    # STT: Chuy·ªÉn audio th√†nh text\n",
        "    user_text = speech_to_text(audio)\n",
        "    if not user_text:\n",
        "        return history, None, \"Kh√¥ng th·ªÉ nh·∫≠n di·ªán gi·ªçng n√≥i. Vui l√≤ng th·ª≠ l·∫°i.\"\n",
        "    \n",
        "    # Th√™m user message v√†o history\n",
        "    history.append({\"role\": \"user\", \"content\": user_text})\n",
        "    \n",
        "    # LLM: T·∫°o response v·ªõi streaming\n",
        "    response_text = \"\"\n",
        "    for partial_text in generate_response_stream(\n",
        "        user_text, history[:-1], system_message, max_tokens, temperature, top_p\n",
        "    ):\n",
        "        response_text = partial_text\n",
        "    \n",
        "    # Th√™m assistant response v√†o history\n",
        "    history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "    \n",
        "    # TTS: Chuy·ªÉn response th√†nh audio\n",
        "    audio_output = text_to_speech(response_text)\n",
        "    \n",
        "    return history, audio_output, response_text\n",
        "\n",
        "\n",
        "def process_text_input(\n",
        "    message,\n",
        "    history: list,\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    \"\"\"\n",
        "    X·ª≠ l√Ω text input: LLM ‚Üí TTS (optional)\n",
        "    \"\"\"\n",
        "    if not message:\n",
        "        return history, None, \"\"\n",
        "    \n",
        "    # Th√™m user message v√†o history\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    \n",
        "    # LLM: T·∫°o response v·ªõi streaming\n",
        "    response_text = \"\"\n",
        "    for partial_text in generate_response_stream(\n",
        "        message, history[:-1], system_message, max_tokens, temperature, top_p\n",
        "    ):\n",
        "        response_text = partial_text\n",
        "    \n",
        "    # Th√™m assistant response v√†o history\n",
        "    history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "    \n",
        "    return history, response_text\n",
        "\n",
        "\n",
        "def chat_with_voice(\n",
        "    audio,\n",
        "    message,\n",
        "    history: list,\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "    enable_tts,\n",
        "):\n",
        "    \"\"\"\n",
        "    H√†m ch√≠nh x·ª≠ l√Ω c·∫£ voice v√† text input\n",
        "    \"\"\"\n",
        "    if audio is not None:\n",
        "        # X·ª≠ l√Ω voice input\n",
        "        history, audio_output, response_text = process_voice_input(\n",
        "            audio, history, system_message, max_tokens, temperature, top_p\n",
        "        )\n",
        "        return history, response_text, audio_output if enable_tts else None\n",
        "    elif message:\n",
        "        # X·ª≠ l√Ω text input\n",
        "        history, response_text = process_text_input(\n",
        "            message, history, system_message, max_tokens, temperature, top_p\n",
        "        )\n",
        "        audio_output = text_to_speech(response_text) if enable_tts else None\n",
        "        return history, response_text, audio_output\n",
        "    else:\n",
        "        return history, \"\", None\n",
        "\n",
        "\n",
        "def stream_chat_response(\n",
        "    message,\n",
        "    history: list,\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    \"\"\"\n",
        "    Stream response cho text chat\n",
        "    \"\"\"\n",
        "    if not message:\n",
        "        return history, \"\"\n",
        "    \n",
        "    # Th√™m user message v√†o history\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    \n",
        "    # Stream response t·ª´ LLM\n",
        "    response_text = \"\"\n",
        "    for partial_text in generate_response_stream(\n",
        "        message, history[:-1], system_message, max_tokens, temperature, top_p\n",
        "    ):\n",
        "        response_text = partial_text\n",
        "        # C·∫≠p nh·∫≠t history v·ªõi partial response\n",
        "        temp_history = history.copy()\n",
        "        temp_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "        yield temp_history, response_text\n",
        "    \n",
        "    # C·∫≠p nh·∫≠t history cu·ªëi c√πng\n",
        "    history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "\n",
        "# T·∫°o Gradio interface\n",
        "with gr.Blocks(title=\"Bank Model Voice Chat\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üè¶ Bank Model Voice Chat\n",
        "    \n",
        "    ·ª®ng d·ª•ng t∆∞ v·∫•n ng√¢n h√†ng v√† t√†i ch√≠nh v·ªõi h·ªó tr·ª£ gi·ªçng n√≥i.\n",
        "    \n",
        "    **Ki·∫øn tr√∫c**: STT (Speech-to-Text) ‚Üí LLM ‚Üí TTS (Text-to-Speech)\n",
        "    \n",
        "    - üé§ **Voice Input**: N√≥i v√†o microphone ƒë·ªÉ ƒë·∫∑t c√¢u h·ªèi\n",
        "    - üí¨ **Text Input**: G√µ c√¢u h·ªèi b·∫±ng vƒÉn b·∫£n\n",
        "    - üîä **Voice Output**: Nghe c√¢u tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i (t√πy ch·ªçn)\n",
        "    \"\"\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"Chat\",\n",
        "                height=500,\n",
        "                show_copy_button=True,\n",
        "                type=\"messages\",\n",
        "            )\n",
        "            \n",
        "            with gr.Row():\n",
        "                audio_input = gr.Audio(\n",
        "                    sources=[\"microphone\"],\n",
        "                    type=\"filepath\",\n",
        "                    label=\"üé§ N√≥i v√†o ƒë√¢y\",\n",
        "                    show_label=True,\n",
        "                )\n",
        "                text_input = gr.Textbox(\n",
        "                    label=\"üí¨ Ho·∫∑c g√µ c√¢u h·ªèi\",\n",
        "                    placeholder=\"Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...\",\n",
        "                    lines=2,\n",
        "                )\n",
        "            \n",
        "            with gr.Row():\n",
        "                submit_voice_btn = gr.Button(\"üé§ G·ª≠i (Voice)\", variant=\"primary\")\n",
        "                submit_text_btn = gr.Button(\"üí¨ G·ª≠i (Text)\", variant=\"primary\")\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è X√≥a l·ªãch s·ª≠\", variant=\"secondary\")\n",
        "        \n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### ‚öôÔ∏è C√†i ƒë·∫∑t\")\n",
        "            \n",
        "            system_msg = gr.Textbox(\n",
        "                value=DEFAULT_SYSTEM_MESSAGE,\n",
        "                label=\"System Message\",\n",
        "                lines=3,\n",
        "            )\n",
        "            \n",
        "            max_tokens = gr.Slider(\n",
        "                minimum=1,\n",
        "                maximum=16384,\n",
        "                value=2048,\n",
        "                step=1,\n",
        "                label=\"Max Tokens\",\n",
        "            )\n",
        "            \n",
        "            temperature = gr.Slider(\n",
        "                minimum=0.1,\n",
        "                maximum=2.0,\n",
        "                value=0.7,\n",
        "                step=0.1,\n",
        "                label=\"Temperature\",\n",
        "            )\n",
        "            \n",
        "            top_p = gr.Slider(\n",
        "                minimum=0.1,\n",
        "                maximum=1.0,\n",
        "                value=0.8,\n",
        "                step=0.05,\n",
        "                label=\"Top-p\",\n",
        "            )\n",
        "            \n",
        "            enable_tts = gr.Checkbox(\n",
        "                value=True,\n",
        "                label=\"üîä B·∫≠t TTS (Text-to-Speech)\",\n",
        "            )\n",
        "            \n",
        "            audio_output = gr.Audio(\n",
        "                label=\"üîä C√¢u tr·∫£ l·ªùi b·∫±ng gi·ªçng n√≥i\",\n",
        "                type=\"filepath\",\n",
        "                autoplay=True,\n",
        "            )\n",
        "            \n",
        "            with gr.Accordion(\"‚ÑπÔ∏è Th√¥ng tin\", open=False):\n",
        "                gr.Markdown(\"\"\"\n",
        "                **Model**: hainguyen306201/bank-model\n",
        "                - Fine-tuned t·ª´ Qwen3-4B-Instruct-2507\n",
        "                - Chuy√™n v·ªÅ t∆∞ v·∫•n ng√¢n h√†ng v√† t√†i ch√≠nh\n",
        "                - H·ªó tr·ª£ ƒëa ng√¥n ng·ªØ (∆∞u ti√™n ti·∫øng Vi·ªát)\n",
        "                \n",
        "                **STT**: OpenAI Whisper (base)\n",
        "                **TTS**: Edge-TTS (vi-VN-HoaiMyNeural)\n",
        "                \"\"\")\n",
        "    \n",
        "    # State ƒë·ªÉ l∆∞u l·ªãch s·ª≠ chat\n",
        "    history_state = gr.State(value=[])\n",
        "    \n",
        "    # Event handlers\n",
        "    def submit_voice(audio, history, system_msg, max_tok, temp, top_p_val, tts_enabled):\n",
        "        if audio is None:\n",
        "            return history, \"\", None, history\n",
        "        new_history, response_text, audio_out = chat_with_voice(\n",
        "            audio, None, history, system_msg, max_tok, temp, top_p_val, tts_enabled\n",
        "        )\n",
        "        return new_history, response_text, audio_out, new_history\n",
        "    \n",
        "    def submit_text_stream(message, history, system_msg, max_tok, temp, top_p_val, tts_enabled):\n",
        "        if not message:\n",
        "            return history, \"\", None, history\n",
        "        \n",
        "        # Th√™m user message v√†o history\n",
        "        history.append({\"role\": \"user\", \"content\": message})\n",
        "        \n",
        "        # Stream response t·ª´ LLM\n",
        "        response_text = \"\"\n",
        "        for partial_text in generate_response_stream(\n",
        "            message, history[:-1], system_msg, max_tok, temp, top_p_val\n",
        "        ):\n",
        "            response_text = partial_text\n",
        "            # C·∫≠p nh·∫≠t history v·ªõi partial response\n",
        "            temp_history = history.copy()\n",
        "            temp_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "            yield temp_history, \"\", None, temp_history\n",
        "        \n",
        "        # C·∫≠p nh·∫≠t history cu·ªëi c√πng\n",
        "        history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "        \n",
        "        # T·∫°o audio n·∫øu TTS ƒë∆∞·ª£c b·∫≠t\n",
        "        audio_out = text_to_speech(response_text) if tts_enabled else None\n",
        "        yield history, \"\", audio_out, history\n",
        "    \n",
        "    def clear_chat():\n",
        "        return [], \"\", None, []\n",
        "    \n",
        "    # Bind events\n",
        "    submit_voice_btn.click(\n",
        "        fn=submit_voice,\n",
        "        inputs=[audio_input, history_state, system_msg, max_tokens, temperature, top_p, enable_tts],\n",
        "        outputs=[chatbot, text_input, audio_output, history_state],\n",
        "    ).then(\n",
        "        fn=lambda: None,\n",
        "        outputs=[audio_input],\n",
        "    )\n",
        "    \n",
        "    submit_text_btn.click(\n",
        "        fn=submit_text_stream,\n",
        "        inputs=[text_input, history_state, system_msg, max_tokens, temperature, top_p, enable_tts],\n",
        "        outputs=[chatbot, text_input, audio_output, history_state],\n",
        "    ).then(\n",
        "        fn=lambda: \"\",\n",
        "        outputs=[text_input],\n",
        "    )\n",
        "    \n",
        "    text_input.submit(\n",
        "        fn=submit_text_stream,\n",
        "        inputs=[text_input, history_state, system_msg, max_tokens, temperature, top_p, enable_tts],\n",
        "        outputs=[chatbot, text_input, audio_output, history_state],\n",
        "    ).then(\n",
        "        fn=lambda: \"\",\n",
        "        outputs=[text_input],\n",
        "    )\n",
        "    \n",
        "    clear_btn.click(\n",
        "        fn=clear_chat,\n",
        "        outputs=[chatbot, text_input, audio_output, history_state],\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ki·ªÉm tra models ƒë√£ ƒë∆∞·ª£c load ch∆∞a\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"KI·ªÇM TRA TR·∫†NG TH√ÅI MODELS...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if model is None:\n",
        "        print(\"‚ùå Model LLM ch∆∞a ƒë∆∞·ª£c t·∫£i!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Model LLM ƒë√£ s·∫µn s√†ng\")\n",
        "    \n",
        "    if tokenizer is None:\n",
        "        print(\"‚ùå Tokenizer ch∆∞a ƒë∆∞·ª£c t·∫£i!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Tokenizer ƒë√£ s·∫µn s√†ng\")\n",
        "    \n",
        "    if whisper_model is None:\n",
        "        print(\"‚ùå Whisper model ch∆∞a ƒë∆∞·ª£c t·∫£i!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Whisper model ƒë√£ s·∫µn s√†ng\")\n",
        "    \n",
        "    if model is None or tokenizer is None or whisper_model is None:\n",
        "        print(\"\\n‚ö†Ô∏è C·∫¢NH B√ÅO: M·ªôt s·ªë models ch∆∞a ƒë∆∞·ª£c t·∫£i. App v·∫´n s·∫Ω ch·∫°y nh∆∞ng c√≥ th·ªÉ g·∫∑p l·ªói.\")\n",
        "        print(\"Vui l√≤ng ƒë·ª£i models ƒë∆∞·ª£c t·∫£i xong ho·∫∑c refresh trang.\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ T·∫§T C·∫¢ MODELS ƒê√É S·∫¥N S√ÄNG! ƒêang kh·ªüi ƒë·ªông ·ª©ng d·ª•ng...\")\n",
        "    \n",
        "    print(\"=\" * 50 + \"\\n\")\n",
        "    \n",
        "    # Ch·∫°y app v·ªõi public link\n",
        "    demo.launch(share=True)\n",
        "'''\n",
        "\n",
        "# Ghi file app.py\n",
        "with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"‚úÖ ƒê√£ t·∫°o file app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import v√† ch·∫°y app\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Th√™m th∆∞ m·ª•c hi·ªán t·∫°i v√†o path\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "# Import app (sau khi ƒë√£ t·∫°o file app.py ·ªü cell tr∆∞·ªõc)\n",
        "try:\n",
        "    from app import demo, model, tokenizer, whisper_model\n",
        "    \n",
        "    # Ki·ªÉm tra models ƒë√£ ƒë∆∞·ª£c t·∫£i ch∆∞a\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"KI·ªÇM TRA TR·∫†NG TH√ÅI MODELS TR∆Ø·ªöC KHI CH·∫†Y APP...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if model is None or tokenizer is None or whisper_model is None:\n",
        "        print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: Models ch∆∞a ƒë∆∞·ª£c t·∫£i xong!\")\n",
        "        print(\"Vui l√≤ng ƒë·ª£i models ƒë∆∞·ª£c t·∫£i xong trong cell tr∆∞·ªõc.\")\n",
        "        print(\"Ki·ªÉm tra output c·ªßa cell t·∫°o app.py ƒë·ªÉ xem ti·∫øn tr√¨nh t·∫£i models.\")\n",
        "    else:\n",
        "        print(\"‚úÖ T·∫§T C·∫¢ MODELS ƒê√É S·∫¥N S√ÄNG!\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"üöÄ ƒêang kh·ªüi ƒë·ªông ·ª©ng d·ª•ng...\")\n",
        "        print(\"=\" * 50 + \"\\n\")\n",
        "        # Ch·∫°y app v·ªõi public link (c√≥ th·ªÉ truy c·∫≠p t·ª´ b√™n ngo√†i)\n",
        "        demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå L·ªói: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\nVui l√≤ng ch·∫°y l·∫°i cell t·∫°o app.py tr∆∞·ªõc\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
